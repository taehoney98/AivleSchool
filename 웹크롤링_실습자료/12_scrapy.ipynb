{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12537ef",
   "metadata": {
    "id": "b12537ef"
   },
   "source": [
    "# Scrapy\n",
    "- 웹사이트에서 데이터 수집을 위한 오픈소스 파이썬 프레임워크\n",
    "- 멀티스레딩으로 데이터 수집\n",
    "- gmarket 상품데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8df8471",
   "metadata": {
    "id": "c8df8471",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\user\\anaconda3\\lib\\site-packages (2.11.1)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (23.10.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (42.0.5)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (24.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (0.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (69.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (23.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (3.2.0)\n",
      "Requirement already satisfied: lxml>=4.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (5.2.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=16.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\user\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: automat>=0.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=22.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Twisted>=18.9.0->scrapy) (4.11.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (2.32.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tldextract->scrapy) (3.13.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "# install scrapy\n",
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0332cd8-37a0-4f19-9a84-e64a565827d6",
   "metadata": {
    "id": "e90ab90f"
   },
   "source": [
    "## 1. make project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf9f5af-145d-428e-9840-ee627b1db8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy , requests\n",
    "from scrapy.http import  TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a5fc97-e7af-4269-9db6-a98211c2dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in C:\\Users\\User\\Desktop\\에이블스쿨\\24.09\\웹크롤링\\웹크롤링_실습자료\\news\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7487a267-a5c0-45a6-b413-2d1429d4d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 PATH의 목록입니다.\n",
      "볼륨 일련 번호는 385B-BB36입니다.\n",
      "C:\\USERS\\USER\\DESKTOP\\에이블스쿨\\24.09\\웹크롤링\\웹크롤링_실습자료\\NEWS\n",
      "│  scrapy.cfg\n",
      "│  \n",
      "└─news\n",
      "    │  items.py\n",
      "    │  middlewares.py\n",
      "    │  pipelines.py\n",
      "    │  settings.py\n",
      "    │  __init__.py\n",
      "    │  \n",
      "    └─spiders\n",
      "            __init__.py\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "!tree news /F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706f47e-415b-4c5d-bdc1-ba4b97a46319",
   "metadata": {
    "id": "cac6776b",
    "tags": []
   },
   "source": [
    "- scrapy structure\n",
    "    - items : 데이터의 모양 정의\n",
    "    - middewares : 수집할때 header 정보와 같은 내용을 설정\n",
    "    - pipelines : 데이터를 수집한 후에 코드를 실행\n",
    "    - settings : robots.txt 규칙, 크롤링 시간 텀등을 설정\n",
    "    - spiders : 크롤링 절차를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580fbf2-7515-4272-8359-deb273acee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "1f6a0691-0464-4f4a-b468-2939a6a1cbf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f2c2301-9562-46fe-aece-481dd73b88c9",
   "metadata": {
    "id": "5f97e6dc",
    "tags": []
   },
   "source": [
    "## 2. xpath\n",
    "- link, contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3aac82cb",
   "metadata": {
    "id": "3aac82cb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scrapy, requests\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ae40418-3b65-4224-b5df-a0f125453e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 https://news.daum.net/>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url= 'https://news.daum.net/'\n",
    "response = requests.get(url)\n",
    "response = TextResponse(response.url, body= response.text , encoding = 'utf-8')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e62e523-22eb-4ee2-8ef6-f7acc806f6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,\n",
       " ['https://v.daum.net/v/20240923151053270',\n",
       "  'https://v.daum.net/v/20240923150750118',\n",
       "  'https://v.daum.net/v/20240923145528413'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = '/html/body/div[2]/main/section/div/div[1]/div[1]/ul/li/div/div/strong/a/@href'\n",
    "links = response.xpath(selector).extract() #data only extract \n",
    "len(links), links[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "968cdb13-a785-4d6f-8c4b-3c0716ee4aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<200 https://v.daum.net/v/20240923151053270>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = links[0]\n",
    "response = requests.get(link)\n",
    "response = TextResponse(response.url, body= response.text , encoding = 'utf-8')\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdcfd075-b704-42d8-955b-615888668958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"전국 주택 2가구 중 1가구 이상은 '준공 20년 이상'\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = response.xpath('//*[@id=\"mArticle\"]/div[1]/h3/text()')[0].extract() # \n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609dd2f8-f3a7-4f86-b53c-a40c8c251e35",
   "metadata": {},
   "source": [
    "### xpath\n",
    "- html element 선택하는 방법\n",
    "- scrapy 에서는 기본적으로 xpath를 사용\n",
    "- syntax\n",
    "    - // : 최상위 엘리먼트\n",
    "    - \\* : 모든 하위 엘리먼트 : css selector의 한칸띄우기와 같다.\n",
    "    - [@id=\"value\"] : 속성값 선택\n",
    "    - / : 한단계 하위 엘리먼트 : css selector의 >와 같다.\n",
    "    - [n] : nth-child(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66594e9-822f-412b-b6ac-5e6d192d943a",
   "metadata": {},
   "source": [
    "## 3. items.py\n",
    "- Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da0a8805-28ee-4deb-9281-ecc84b0b68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load news/news/items.py\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NewsItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36c20c-a58d-4f60-8ed4-b13178e174d7",
   "metadata": {
    "id": "4d0aa7e1",
    "tags": []
   },
   "source": [
    "## 4. spider.py\n",
    "- wirte crawling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cdb3e0db-4ff6-4ec7-aa24-b8ae37feff18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting news/news/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile news/news/spiders/spider.py\n",
    "\n",
    "import scrapy\n",
    "from news.items import NewsItem # newsitem written above\n",
    "\n",
    "\n",
    "class newsSpider(scrapy.Spider):\n",
    "    name = 'news'\n",
    "    allow_domain = ['daum.net'] # only crawling daum site\n",
    "    start_urls = ['https://news.daum.net'] # 3inital variables\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "        selector = '/html/body/div[2]/main/section/div/div[1]/div[1]/ul/li/div/div/strong/a/@href'\n",
    "        links = response.xpath(selector).extract()\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link,callback=self.parse_content)\n",
    "\n",
    "\n",
    "\n",
    "    def parse_content(self, response):\n",
    "        item = NewsItem()\n",
    "        item['link'] = response.url\n",
    "        item['title'] = response.xpath('//*[@id=\"mArticle\"]/div[1]/h3/text()')[0].extract()\n",
    "        yield item\n",
    "\n",
    "\n",
    "##  yiled는 여러번 return 수행한다고 생각하면 편함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46f1ea6e-3283-426e-8d9b-7eb568fc8300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo():\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2acea0-0999-4cb6-943d-7588f92b29f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1927d7-d0b6-42e7-ba84-79eb8674dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo():\n",
    "    yield 1\n",
    "    yield 2\n",
    "    yield 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d808cb96-efd3-4c4f-8330-e24b44eee476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object echo at 0x000002C8AF7FD6F0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 제너레이터 : 값 생성기\n",
    "echo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73322f8b-08b9-4428-97e6-4703546df736",
   "metadata": {},
   "outputs": [],
   "source": [
    "e =echo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bde818f6-0841-4c2a-954a-ba829ff9a91d",
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mnext\u001b[39m(e)\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(e) # 순서대로 값 나오고 ERROR 발"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d4d2e-1978-4d8c-ac50-0ca52803c670",
   "metadata": {
    "id": "bd2e1c16",
    "tags": []
   },
   "source": [
    "## 5. run scrapy\n",
    "- news 디렉토리에서 아래의 커멘드 실행\n",
    "- scrapy crawl news(name) -o news.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8cb58aeb-5e06-47d5-aee2-4370f99d6d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Desktop\\\\에이블스쿨\\\\24.09\\\\웹크롤링\\\\웹크롤링_실습자료'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811644a-184c-46f7-b2f3-ceda17dac532",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl news -o news.csv"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "04_scrapy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
